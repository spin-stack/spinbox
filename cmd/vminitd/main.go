//go:build linux

package main

import (
	"context"
	"errors"
	"flag"
	"fmt"
	"net"
	"os"
	"os/signal"
	"runtime"
	"strings"
	"time"

	"github.com/containerd/containerd/v2/core/mount"
	"github.com/containerd/containerd/v2/pkg/shutdown"
	"github.com/containerd/containerd/v2/pkg/sys/reaper"
	cplugins "github.com/containerd/containerd/v2/plugins"
	"github.com/containerd/log"
	"github.com/containerd/otelttrpc"
	"github.com/containerd/plugin"
	"github.com/containerd/plugin/registry"
	"github.com/containerd/ttrpc"
	"github.com/mdlayher/vsock"
	"golang.org/x/sys/unix"

	"github.com/aledbf/beacon/containerd/vminit"
	"github.com/aledbf/beacon/containerd/vminit/systools"

	_ "github.com/aledbf/beacon/containerd/services"
	_ "github.com/aledbf/beacon/containerd/vminit/events"
	_ "github.com/aledbf/beacon/containerd/vminit/streaming"
)

func main() {
	var config ServiceConfig
	flag.BoolVar(&config.Debug, "debug", false, "Debug log level")
	flag.IntVar(&config.RPCPort, "vsock-rpc-port", 1025, "vsock port to listen for rpc on")
	flag.IntVar(&config.StreamPort, "vsock-stream-port", 1026, "vsock port to listen for streams on")
	flag.IntVar(&config.VSockContextID, "vsock-cid", 3, "vsock context ID for vsock listen")
	args := os.Args[1:]

	flag.CommandLine.Parse(args)

	if config.Debug {
		log.SetLevel("debug")
	}

	ctx := context.Background()

	log.G(ctx).WithField("args", args).WithField("env", os.Environ()).Debug("starting vminitd")

	defer func() {
		if p := recover(); p != nil {
			log.G(ctx).WithField("panic", p).Error("recovered from panic")
		}

		// Trigger VM shutdown via reboot syscall
		// This will cause Cloud Hypervisor to exit cleanly
		log.G(ctx).Info("powering off VM")
		if err := unix.Reboot(unix.LINUX_REBOOT_CMD_POWER_OFF); err != nil {
			log.G(ctx).WithError(err).Error("failed to power off VM")
		}
	}()

	if err := run(ctx, config); err != nil {
		log.G(ctx).WithError(err).Error("exiting with error")
	}
}

func run(ctx context.Context, config ServiceConfig) error {
	t1 := time.Now()

	ctx, config.Shutdown = shutdown.WithShutdown(ctx)

	if err := systemInit(ctx, config); err != nil {
		return err
	}

	if config.Debug {
		systools.DumpInfo(ctx)
	}

	service, err := New(ctx, config)
	if err != nil {
		return err
	}

	log.G(ctx).WithField("t", time.Since(t1)).Debug("initialized vminitd")

	// Limit GOMAXPROCS to 2 for VM environment
	// Cloud Hypervisor VMs typically have 2 vCPUs allocated
	// This prevents scheduler overhead and improves cache locality
	runtime.GOMAXPROCS(2)

	serviceErr := make(chan error, 1)
	go func() {
		serviceErr <- service.Run(ctx)
	}()

	s := make(chan os.Signal, 1)
	signal.Notify(s, unix.SIGKILL, unix.SIGINT, unix.SIGTERM, unix.SIGHUP, unix.SIGQUIT, unix.SIGCHLD)
	for {
		select {
		case <-config.Shutdown.Done():
			if err := config.Shutdown.Err(); err != nil && !errors.Is(err, shutdown.ErrShutdown) {
				log.G(ctx).WithError(err).Error("shutdown error")
			}
			return nil
		case err := <-serviceErr:
			log.G(ctx).WithError(err).Error("service exited")
			return err
		case sig := <-s:
			switch sig {
			case unix.SIGCHLD:
				if err := reaper.Reap(); err != nil {
					log.G(ctx).WithError(err).Error("failed to reap child process")
				} else {
					log.G(ctx).Debug("reaped child process")
				}
			case unix.SIGKILL, unix.SIGINT, unix.SIGTERM, unix.SIGQUIT:
				config.Shutdown.Shutdown()
				log.G(ctx).WithField("signal", sig).Info("received shutdown signal")
			default:
				log.G(ctx).WithField("signal", sig).Debug("received unhandled signal")
			}
		}
	}
}

// systemInit initializes the system
func systemInit(ctx context.Context, _ ServiceConfig) error {
	if err := systemMounts(); err != nil {
		return err
	}

	// Wait for virtio block devices to appear
	// This is necessary because the kernel may not have probed all virtio devices yet
	// Not fatal if devices don't appear - they might appear later or not be needed
	waitForBlockDevices(ctx)

	if err := setupCgroupControl(); err != nil {
		return err
	}

	if err := os.Mkdir("/etc", 0755); err != nil && !os.IsExist(err) {
		return fmt.Errorf("failed to create /etc: %w", err)
	}

	return nil
}

// waitForBlockDevices waits for virtio block devices to appear in /dev
// The kernel needs time to probe PCI devices and create device nodes
// This is a best-effort operation - if devices don't appear, we continue anyway
func waitForBlockDevices(ctx context.Context) {
	timeout := 5 * time.Second
	pollInterval := 10 * time.Millisecond

	log.G(ctx).Debug("waiting for virtio block devices to appear")

	ctx, cancel := context.WithTimeout(ctx, timeout)
	defer cancel()

	ticker := time.NewTicker(pollInterval)
	defer ticker.Stop()

	for {
		// Check /sys/block for all block devices
		entries, err := os.ReadDir("/sys/block")
		if err == nil {
			var vdDevices []string
			for _, entry := range entries {
				if strings.HasPrefix(entry.Name(), "vd") {
					vdDevices = append(vdDevices, entry.Name())
				}
			}

			if len(vdDevices) > 0 {
				log.G(ctx).WithField("devices", vdDevices).Info("found virtio block devices in /sys/block")

				// Wait for /dev nodes to appear
				time.Sleep(100 * time.Millisecond)

				// Verify /dev nodes exist
				var devNodes []string
				for _, dev := range vdDevices {
					devPath := "/dev/" + dev
					if _, err := os.Stat(devPath); err == nil {
						devNodes = append(devNodes, devPath)
					}
				}

				if len(devNodes) > 0 {
					log.G(ctx).WithField("dev_nodes", devNodes).Info("virtio block device nodes ready")
					return
				}
			}
		}

		select {
		case <-ctx.Done():
			log.G(ctx).Warn("timeout waiting for virtio block devices, continuing anyway")
			return
		case <-ticker.C:
			// continue polling
		}
	}
}

func systemMounts() error {
	// Create /lib if it doesn't exist (needed for modules)
	if err := os.MkdirAll("/lib", 0755); err != nil && !os.IsExist(err) {
		return fmt.Errorf("failed to create /lib: %w", err)
	}

	return mount.All([]mount.Mount{
		{
			Type:    "proc",
			Source:  "proc",
			Target:  "/proc",
			Options: []string{"nosuid", "noexec", "nodev"},
		},
		{
			Type:    "sysfs",
			Source:  "sysfs",
			Target:  "/sys",
			Options: []string{"nosuid", "noexec", "nodev"},
		},
		{
			Type:   "cgroup2",
			Source: "none",
			Target: "/sys/fs/cgroup",
		},
		{
			Type:    "tmpfs",
			Source:  "tmpfs",
			Target:  "/run",
			Options: []string{"nosuid", "noexec", "nodev"},
		},
		{
			Type:    "tmpfs",
			Source:  "tmpsfs",
			Target:  "/tmp",
			Options: []string{"nosuid", "noexec", "nodev"},
		},
		{
			Type:    "devtmpfs",
			Source:  "devtmpsfs",
			Target:  "/dev",
			Options: []string{"nosuid", "noexec"},
		},
	}, "/")
}

func setupCgroupControl() error {
	return os.WriteFile("/sys/fs/cgroup/cgroup.subtree_control", []byte("+cpu +cpuset +io +memory +pids"), 0644)
}

// ttrpcService allows TTRPC services to be registered with the underlying server
type ttrpcService interface {
	RegisterTTRPC(*ttrpc.Server) error
}

type service struct {
	l      net.Listener
	server *ttrpc.Server
}

type Runnable interface {
	Run(context.Context) error
}

type ServiceConfig struct {
	VSockContextID  int
	RPCPort         int
	StreamPort      int
	Shutdown        shutdown.Service
	Debug           bool
	DisabledPlugins []string
}

func New(ctx context.Context, config ServiceConfig) (Runnable, error) {
	var (
		initializedPlugins = plugin.NewPluginSet()
		disabledPlugins    = map[string]struct{}{}
	)

	// Build disabled plugins map from config
	if len(config.DisabledPlugins) > 0 {
		for _, p := range config.DisabledPlugins {
			disabledPlugins[p] = struct{}{}
		}
	}

	l, err := vsock.ListenContextID(uint32(config.VSockContextID), uint32(config.RPCPort), &vsock.Config{})
	if err != nil {
		return nil, fmt.Errorf("failed to listen on vsock port %d with context id %d: %w", config.RPCPort, config.VSockContextID, err)
	}
	config.Shutdown.RegisterCallback(func(ctx context.Context) error {
		return l.Close()
	})

	ts, err := ttrpc.NewServer(
		ttrpc.WithUnaryServerInterceptor(otelttrpc.UnaryServerInterceptor()),
	)
	if err != nil {
		return nil, err
	}
	config.Shutdown.RegisterCallback(ts.Shutdown)

	registry.Register(&plugin.Registration{
		Type: cplugins.InternalPlugin,
		ID:   "shutdown",
		InitFn: func(ic *plugin.InitContext) (any, error) {
			return config.Shutdown, nil
		},
	})

	for _, reg := range registry.Graph(func(*plugin.Registration) bool { return false }) {
		id := reg.URI()
		if _, ok := disabledPlugins[id]; ok {
			log.G(ctx).WithField("plugin_id", id).Info("plugin is disabled, skipping load")
			continue
		}

		log.G(ctx).WithField("plugin_id", id).Info("loading plugin")

		ic := plugin.NewContext(ctx, initializedPlugins, nil)

		if reg.Config != nil {
			// TODO: Allow plugin config?
			if vc, ok := reg.Config.(interface{ SetVsock(uint32, uint32) }); ok {
				if reg.Type == vminit.StreamingPlugin {
					vc.SetVsock(uint32(config.VSockContextID), uint32(config.StreamPort))
				}
			}

			ic.Config = reg.Config
		}

		p := reg.Init(ic)
		if err := initializedPlugins.Add(p); err != nil {
			return nil, fmt.Errorf("could not add plugin result to plugin set: %w", err)
		}

		instance, err := p.Instance()
		if err != nil {
			if plugin.IsSkipPlugin(err) {
				log.G(ctx).WithFields(log.Fields{"error": err, "plugin_id": id}).Info("skip loading plugin")
				continue
			}

			return nil, fmt.Errorf("failed to load plugin %s: %w", id, err)
		}

		if s, ok := instance.(ttrpcService); ok {
			s.RegisterTTRPC(ts)
		}
	}

	return &service{
		l:      l,
		server: ts,
	}, nil
}

func (s *service) Run(ctx context.Context) error {
	return s.server.Serve(ctx, s.l)
}
