//go:build linux

package main

import (
	"context"
	"encoding/json"
	"errors"
	"flag"
	"fmt"
	"net"
	"os"
	"os/signal"
	"runtime"
	"strings"
	"time"

	"github.com/containerd/containerd/v2/core/mount"
	"github.com/containerd/containerd/v2/pkg/shutdown"
	"github.com/containerd/containerd/v2/pkg/sys/reaper"
	cplugins "github.com/containerd/containerd/v2/plugins"
	"github.com/containerd/log"
	"github.com/containerd/otelttrpc"
	"github.com/containerd/plugin"
	"github.com/containerd/plugin/registry"
	"github.com/containerd/ttrpc"
	"github.com/mdlayher/vsock"
	"golang.org/x/sys/unix"

	"github.com/aledbf/qemubox/containerd/internal/guest/vminit"
	"github.com/aledbf/qemubox/containerd/internal/guest/vminit/systools"

	_ "github.com/aledbf/qemubox/containerd/internal/guest/services"
	_ "github.com/aledbf/qemubox/containerd/internal/guest/vminit/events"
	_ "github.com/aledbf/qemubox/containerd/internal/guest/vminit/streaming"
)

// loadConfig loads configuration from a JSON file and merges it with the provided config.
// Command-line flags take precedence over file configuration.
func loadConfig(path string, config *ServiceConfig) error {
	data, err := os.ReadFile(path)
	if err != nil {
		return fmt.Errorf("failed to read config file: %w", err)
	}

	// Store flag values before unmarshaling
	flagDebug := config.Debug
	flagRPCPort := config.RPCPort
	flagStreamPort := config.StreamPort
	flagVSockContextID := config.VSockContextID

	if err := json.Unmarshal(data, config); err != nil {
		return fmt.Errorf("failed to parse config file: %w", err)
	}

	// Restore flag values if they were set (non-zero/non-default)
	// This ensures flags override config file
	if flagDebug {
		config.Debug = flagDebug
	}
	if flagRPCPort != 0 && flagRPCPort != 1025 {
		config.RPCPort = flagRPCPort
	}
	if flagStreamPort != 0 && flagStreamPort != 1026 {
		config.StreamPort = flagStreamPort
	}
	if flagVSockContextID != 0 && flagVSockContextID != 3 {
		config.VSockContextID = flagVSockContextID
	}

	return nil
}

// applyPluginConfig applies configuration map to a plugin config struct.
// This is a simple implementation that marshals the map to JSON and then
// unmarshals it into the config struct, which handles type conversions.
func applyPluginConfig(pluginConfig any, configMap map[string]any) error {
	if pluginConfig == nil {
		return fmt.Errorf("plugin config is nil")
	}

	// Marshal the config map to JSON
	data, err := json.Marshal(configMap)
	if err != nil {
		return fmt.Errorf("failed to marshal config map: %w", err)
	}

	// Unmarshal into the plugin config struct
	if err := json.Unmarshal(data, pluginConfig); err != nil {
		return fmt.Errorf("failed to unmarshal into plugin config: %w", err)
	}

	return nil
}

func main() {
	var (
		config     ServiceConfig
		configFile string
	)
	flag.StringVar(&configFile, "config", "", "Path to configuration file")
	flag.BoolVar(&config.Debug, "debug", false, "Debug log level")
	flag.IntVar(&config.RPCPort, "vsock-rpc-port", 1025, "vsock port to listen for rpc on")
	flag.IntVar(&config.StreamPort, "vsock-stream-port", 1026, "vsock port to listen for streams on")
	flag.IntVar(&config.VSockContextID, "vsock-cid", 3, "vsock context ID for vsock listen")
	args := os.Args[1:]

	flag.CommandLine.Parse(args)

	// Load configuration file if provided
	if configFile != "" {
		if err := loadConfig(configFile, &config); err != nil {
			log.L.WithError(err).Fatalf("failed to load config from %s", configFile)
		}
	}

	if config.Debug {
		log.SetLevel("debug")
	} else {
		// Prefer verbose logging in the minimal VM to ease debugging boot/mount issues.
		log.SetLevel("info")
	}

	ctx := context.Background()

	log.G(ctx).WithField("args", args).WithField("env", os.Environ()).Debug("starting vminitd")

	defer func() {
		if p := recover(); p != nil {
			log.G(ctx).WithField("panic", p).Error("recovered from panic")
		}

		// Trigger VM shutdown via reboot syscall
		// This will cause QEMU to exit cleanly
		log.G(ctx).Info("powering off VM")
		if err := unix.Reboot(unix.LINUX_REBOOT_CMD_POWER_OFF); err != nil {
			log.G(ctx).WithError(err).Error("failed to power off VM")
		}
	}()

	if err := run(ctx, config); err != nil {
		log.G(ctx).WithError(err).Error("exiting with error")
	}
}

func run(ctx context.Context, config ServiceConfig) error {
	t1 := time.Now()

	ctx, config.Shutdown = shutdown.WithShutdown(ctx)

	if err := systemInit(ctx, config); err != nil {
		return err
	}

	if config.Debug {
		systools.DumpInfo(ctx)
	}

	service, err := New(ctx, config)
	if err != nil {
		return err
	}

	log.G(ctx).WithField("t", time.Since(t1)).Debug("initialized vminitd")

	// Limit GOMAXPROCS to 2 for VM environment
	// QEMU VMs typically have configurable vCPUs allocated
	// This prevents scheduler overhead and improves cache locality
	runtime.GOMAXPROCS(2)

	serviceErr := make(chan error, 1)
	go func() {
		serviceErr <- service.Run(ctx)
	}()

	s := make(chan os.Signal, 1)
	signal.Notify(s, unix.SIGKILL, unix.SIGINT, unix.SIGTERM, unix.SIGHUP, unix.SIGQUIT, unix.SIGCHLD)
	for {
		select {
		case <-config.Shutdown.Done():
			if err := config.Shutdown.Err(); err != nil && !errors.Is(err, shutdown.ErrShutdown) {
				log.G(ctx).WithError(err).Error("shutdown error")
			}
			return nil
		case err := <-serviceErr:
			log.G(ctx).WithError(err).Error("service exited")
			return err
		case sig := <-s:
			switch sig {
			case unix.SIGCHLD:
				if err := reaper.Reap(); err != nil {
					log.G(ctx).WithError(err).Error("failed to reap child process")
				} else {
					log.G(ctx).Debug("reaped child process")
				}
			case unix.SIGKILL, unix.SIGINT, unix.SIGTERM, unix.SIGQUIT:
				config.Shutdown.Shutdown()
				log.G(ctx).WithField("signal", sig).Info("received shutdown signal")
			default:
				log.G(ctx).WithField("signal", sig).Debug("received unhandled signal")
			}
		}
	}
}

// systemInit initializes the system
func systemInit(ctx context.Context, _ ServiceConfig) error {
	if err := systemMounts(); err != nil {
		return err
	}

	// Wait for virtio block devices to appear
	// This is necessary because the kernel may not have probed all virtio devices yet
	// Not fatal if devices don't appear - they might appear later or not be needed
	waitForBlockDevices(ctx)

	if err := setupCgroupControl(); err != nil {
		return err
	}

	if err := os.Mkdir("/etc", 0755); err != nil && !os.IsExist(err) {
		return fmt.Errorf("failed to create /etc: %w", err)
	}

	// Configure DNS from kernel command line
	if err := configureDNS(ctx); err != nil {
		log.G(ctx).WithError(err).Warn("failed to configure DNS, continuing anyway")
	}

	return nil
}

// waitForBlockDevices waits for virtio block devices to appear in /dev
// The kernel needs time to probe PCI devices and create device nodes
// This is a best-effort operation - if devices don't appear, we continue anyway
func waitForBlockDevices(ctx context.Context) {
	timeout := 5 * time.Second
	pollInterval := 10 * time.Millisecond

	log.G(ctx).Debug("waiting for virtio block devices to appear")

	ctx, cancel := context.WithTimeout(ctx, timeout)
	defer cancel()

	ticker := time.NewTicker(pollInterval)
	defer ticker.Stop()

	for {
		// Check /sys/block for all block devices
		entries, err := os.ReadDir("/sys/block")
		if err == nil {
			var vdDevices []string
			for _, entry := range entries {
				if strings.HasPrefix(entry.Name(), "vd") {
					vdDevices = append(vdDevices, entry.Name())
				}
			}

			if len(vdDevices) > 0 {
				log.G(ctx).WithField("devices", vdDevices).Info("found virtio block devices in /sys/block")

				// Wait for /dev nodes to appear
				time.Sleep(100 * time.Millisecond)

				// Verify /dev nodes exist
				var devNodes []string
				for _, dev := range vdDevices {
					devPath := "/dev/" + dev
					if _, err := os.Stat(devPath); err == nil {
						devNodes = append(devNodes, devPath)
					}
				}

				if len(devNodes) > 0 {
					log.G(ctx).WithField("dev_nodes", devNodes).Info("virtio block device nodes ready")
					return
				}
			}
		}

		select {
		case <-ctx.Done():
			log.G(ctx).Warn("timeout waiting for virtio block devices, continuing anyway")
			return
		case <-ticker.C:
			// continue polling
		}
	}
}

func systemMounts() error {
	// Create /lib if it doesn't exist (needed for modules)
	if err := os.MkdirAll("/lib", 0755); err != nil && !os.IsExist(err) {
		return fmt.Errorf("failed to create /lib: %w", err)
	}

	return mount.All([]mount.Mount{
		{
			Type:    "proc",
			Source:  "proc",
			Target:  "/proc",
			Options: []string{"nosuid", "noexec", "nodev"},
		},
		{
			Type:    "sysfs",
			Source:  "sysfs",
			Target:  "/sys",
			Options: []string{"nosuid", "noexec", "nodev"},
		},
		{
			Type:   "cgroup2",
			Source: "none",
			Target: "/sys/fs/cgroup",
		},
		{
			Type:    "tmpfs",
			Source:  "tmpfs",
			Target:  "/run",
			Options: []string{"nosuid", "noexec", "nodev"},
		},
		{
			Type:    "tmpfs",
			Source:  "tmpsfs",
			Target:  "/tmp",
			Options: []string{"nosuid", "noexec", "nodev"},
		},
		{
			Type:    "devtmpfs",
			Source:  "devtmpsfs",
			Target:  "/dev",
			Options: []string{"nosuid", "noexec"},
		},
	}, "/")
}

func setupCgroupControl() error {
	return os.WriteFile("/sys/fs/cgroup/cgroup.subtree_control", []byte("+cpu +cpuset +io +memory +pids"), 0644)
}

// configureDNS parses DNS servers from kernel ip= parameter and writes /etc/resolv.conf
// The kernel ip= parameter format is:
// ip=<client-ip>:<server-ip>:<gw-ip>:<netmask>:<hostname>:<device>:<autoconf>:<dns0-ip>:<dns1-ip>
func configureDNS(ctx context.Context) error {
	// Read kernel command line
	cmdlineBytes, err := os.ReadFile("/proc/cmdline")
	if err != nil {
		return fmt.Errorf("failed to read /proc/cmdline: %w", err)
	}

	cmdline := string(cmdlineBytes)
	log.G(ctx).WithField("cmdline", cmdline).Debug("parsing kernel command line for DNS config")

	// Parse ip= parameter
	var nameservers []string
	for _, param := range strings.Fields(cmdline) {
		if strings.HasPrefix(param, "ip=") {
			ipParam := strings.TrimPrefix(param, "ip=")
			// Split by colons: client-ip:server-ip:gw-ip:netmask:hostname:device:autoconf:dns0-ip:dns1-ip
			parts := strings.Split(ipParam, ":")

			// DNS servers are at index 7 and 8 (0-indexed)
			// Format: ip=<client-ip>:<server-ip>:<gw-ip>:<netmask>:<hostname>:<device>:<autoconf>:<dns0-ip>:<dns1-ip>
			//         0           1           2      3         4          5        6           7         8
			if len(parts) > 7 && parts[7] != "" {
				nameservers = append(nameservers, parts[7])
			}
			if len(parts) > 8 && parts[8] != "" {
				nameservers = append(nameservers, parts[8])
			}
			break
		}
	}

	if len(nameservers) == 0 {
		log.G(ctx).Debug("no DNS servers found in kernel ip= parameter")
		return nil
	}

	// Build resolv.conf content
	var resolvConf strings.Builder
	for _, ns := range nameservers {
		fmt.Fprintf(&resolvConf, "nameserver %s\n", ns)
	}

	// Write /etc/resolv.conf
	if err := os.WriteFile("/etc/resolv.conf", []byte(resolvConf.String()), 0644); err != nil {
		return fmt.Errorf("failed to write /etc/resolv.conf: %w", err)
	}

	log.G(ctx).WithField("nameservers", nameservers).Info("configured DNS resolvers from kernel ip= parameter")
	return nil
}

// ttrpcService allows TTRPC services to be registered with the underlying server
type ttrpcService interface {
	RegisterTTRPC(*ttrpc.Server) error
}

type service struct {
	l      net.Listener
	server *ttrpc.Server
}

type Runnable interface {
	Run(context.Context) error
}

type ServiceConfig struct {
	VSockContextID  int                       `json:"vsock_context_id,omitempty"`
	RPCPort         int                       `json:"rpc_port,omitempty"`
	StreamPort      int                       `json:"stream_port,omitempty"`
	Shutdown        shutdown.Service          `json:"-"`
	Debug           bool                      `json:"debug,omitempty"`
	DisabledPlugins []string                  `json:"disabled_plugins,omitempty"`
	PluginConfigs   map[string]map[string]any `json:"plugin_configs,omitempty"`
}

func New(ctx context.Context, config ServiceConfig) (Runnable, error) {
	var (
		initializedPlugins = plugin.NewPluginSet()
		disabledPlugins    = map[string]struct{}{}
	)

	// Build disabled plugins map from config
	if len(config.DisabledPlugins) > 0 {
		for _, p := range config.DisabledPlugins {
			disabledPlugins[p] = struct{}{}
		}
	}

	l, err := vsock.ListenContextID(uint32(config.VSockContextID), uint32(config.RPCPort), &vsock.Config{})
	if err != nil {
		return nil, fmt.Errorf("failed to listen on vsock port %d with context id %d: %w", config.RPCPort, config.VSockContextID, err)
	}
	log.G(ctx).WithFields(log.Fields{
		"cid":  config.VSockContextID,
		"port": config.RPCPort,
	}).Info("listening on vsock for RPC connections")
	config.Shutdown.RegisterCallback(func(ctx context.Context) error {
		return l.Close()
	})

	ts, err := ttrpc.NewServer(
		ttrpc.WithUnaryServerInterceptor(otelttrpc.UnaryServerInterceptor()),
	)
	if err != nil {
		return nil, err
	}
	config.Shutdown.RegisterCallback(ts.Shutdown)

	registry.Register(&plugin.Registration{
		Type: cplugins.InternalPlugin,
		ID:   "shutdown",
		InitFn: func(ic *plugin.InitContext) (any, error) {
			return config.Shutdown, nil
		},
	})

	for _, reg := range registry.Graph(func(*plugin.Registration) bool { return false }) {
		id := reg.URI()
		if _, ok := disabledPlugins[id]; ok {
			log.G(ctx).WithField("plugin_id", id).Info("plugin is disabled, skipping load")
			continue
		}

		log.G(ctx).WithField("plugin_id", id).Info("loading plugin")

		ic := plugin.NewContext(ctx, initializedPlugins, nil)

		if reg.Config != nil {
			// Apply plugin-specific configuration from config file if available
			if pluginCfg, ok := config.PluginConfigs[id]; ok {
				// Attempt to merge plugin config
				// This uses reflection to set fields, assuming Config is a pointer to struct
				if err := applyPluginConfig(reg.Config, pluginCfg); err != nil {
					log.G(ctx).WithFields(log.Fields{
						"plugin_id": id,
						"error":     err,
					}).Warn("failed to apply plugin configuration")
				}
			}

			if vc, ok := reg.Config.(interface{ SetVsock(uint32, uint32) }); ok {
				if reg.Type == vminit.StreamingPlugin {
					vc.SetVsock(uint32(config.VSockContextID), uint32(config.StreamPort))
				}
			}

			ic.Config = reg.Config
		}

		p := reg.Init(ic)
		if err := initializedPlugins.Add(p); err != nil {
			return nil, fmt.Errorf("could not add plugin result to plugin set: %w", err)
		}

		instance, err := p.Instance()
		if err != nil {
			if plugin.IsSkipPlugin(err) {
				log.G(ctx).WithFields(log.Fields{"error": err, "plugin_id": id}).Info("skip loading plugin")
				continue
			}

			return nil, fmt.Errorf("failed to load plugin %s: %w", id, err)
		}

		if s, ok := instance.(ttrpcService); ok {
			s.RegisterTTRPC(ts)
		}
	}

	return &service{
		l:      l,
		server: ts,
	}, nil
}

func (s *service) Run(ctx context.Context) error {
	log.G(ctx).Info("starting TTRPC server")
	return s.server.Serve(ctx, s.l)
}
